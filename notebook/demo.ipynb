{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deployment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "##### NO NEED TO RUN\n",
        "##### This cell used to prepare and export 'data' and 'model'\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# pip install sentence-transformers sklearn dash numpy gunicorn\n",
        "\n",
        "## EXPORT movie_lists AND document_embeddings\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "# tmp = pd.read_csv('./movie_vectors/output.csv')\n",
        "# import pickle\n",
        "# import json\n",
        "\n",
        "# with open('./movie_vectors/movie_lists', 'wb') as fp:\n",
        "#   movie_lists = tmp.title.values\n",
        "#   pickle.dump(movie_lists, fp)\n",
        "\n",
        "# with open('./movie_vectors/document_embeddings', 'wb') as fp:\n",
        "#   document_embeddings = [json.loads(x) for x in tmp.document_embeddings.values]\n",
        "#   pickle.dump(document_embeddings, fp)\n",
        "\n",
        "# with open('./movie_vectors/pairwise_similarities', 'wb') as fp:\n",
        "#   pairwise_similarities = cosine_similarity(document_embeddings)\n",
        "#   pickle.dump(pairwise_similarities, fp)\n",
        "\n",
        "# with open('./others/stopwords', 'wb') as fp:\n",
        "#   stopwords_english = stopwords.words('english')\n",
        "#   pickle.dump(stopwords_english, fp)\n",
        "\n",
        "# with open('model.pickle', 'wb') as pkl:\n",
        "#   sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "#   pickle.dump(sbert_model, pkl)\n",
        "\n",
        "\n",
        "# import requests\n",
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('./tmdb_5000_movies.csv')\n",
        "# poster_urls = []\n",
        "# for movie_id in df.id.values:\n",
        "#   url = \"https://api.themoviedb.org/3/movie/{}?api_key=8265bd1679663a7ea12ac168da84d2e8&language=en-US\".format(movie_id)\n",
        "#   data = requests.get(url)\n",
        "#   data = data.json()\n",
        "#   try:\n",
        "#     poster_path = data['poster_path']\n",
        "#     full_path = \"https://image.tmdb.org/t/p/w500\" + poster_path\n",
        "#   except:\n",
        "#     full_path = \"\"\n",
        "#   poster_urls.append(full_path)\n",
        "\n",
        "# with open('./data/poster_urls_lists', 'wb') as fp:\n",
        "#   pickle.dump(np.array(poster_urls), fp)"
      ],
      "metadata": {
        "id": "5lC98WUrjCZR"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyter-dash sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMEgEQCUsC1f",
        "outputId": "bf6cc3b4-41db-4eb7-fb60-8c0b30bceae7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jupyter-dash in /usr/local/lib/python3.7/dist-packages (0.4.2)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (1.1.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (4.10.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (1.5.5)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (1.3.3)\n",
            "Requirement already satisfied: ansi2html in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (1.7.0)\n",
            "Requirement already satisfied: dash in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (2.3.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (2.23.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 20.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 45.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 48.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 44.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.8)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash->jupyter-dash) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.7/dist-packages (from dash->jupyter-dash) (5.0.0)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.7/dist-packages (from dash->jupyter-dash) (2.0.0)\n",
            "Requirement already satisfied: flask-compress in /usr/local/lib/python3.7/dist-packages (from dash->jupyter-dash) (1.12)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from dash->jupyter-dash) (5.5.0)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->jupyter-dash) (2.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (8.0.1)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.7/dist-packages (from flask-compress->dash->jupyter-dash) (1.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter-dash) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter-dash) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter-dash) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyter-dash) (0.2.5)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (4.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (2.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->jupyter-dash) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=1dc1378e1ffc41a190aab1b0eeb01db638596c61896b3e8b32b0cb19819de6b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=80c3c99cafab29c6ef7a353c9a2a8dde81daf1e83ddadb7a37405534c1e901ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 sentence-transformers-2.2.0 sentencepiece-0.1.96 tokenizers-0.12.1 transformers-4.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# LIBRARIES\n",
        "##############################################\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "from jupyter_dash import JupyterDash\n",
        "from dash import dcc, html\n",
        "from dash.dependencies import Input, Output, State\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "kXfL7FT8sC3p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# FUNCTIONS\n",
        "##############################################\n",
        "\n",
        "def prepare_text(x):\n",
        "  return \" \".join(re.sub(r'[^a-zA-Z]',' ',w).lower() for w in x.split() if re.sub(r'[^a-zA-Z]',' ',w).lower() not in stop_words_l)\n"
      ],
      "metadata": {
        "id": "_YTqt6ED8Uts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################\n",
        "# DATA\n",
        "##############################################\n",
        "\n",
        "with open('./data/stopwords', 'rb') as fp:\n",
        "  stop_words_l = pickle.load(fp)\n",
        "\n",
        "with open('./data/movie_lists', 'rb') as fp:\n",
        "  movie_lists = pickle.load(fp)\n",
        "\n",
        "with open('./data/poster_urls_lists', 'rb') as fp:\n",
        "  poster_lists = pickle.load(fp)\n",
        "\n",
        "with open('./data/document_embeddings', 'rb') as fp:\n",
        "  document_embeddings = pickle.load(fp)\n",
        "\n",
        "pairwise_similarities = cosine_similarity(document_embeddings)\n",
        "\n",
        "# The model.pickle file wasn't uploaded to Github due to file size.\n",
        "# But those who wants to run this the demo can use the code below.\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# This is for those who has model.pickle file.\n",
        "with open('./model/model.pickle', 'rb') as fp:\n",
        "  sbert_model = pickle.load(fp)"
      ],
      "metadata": {
        "id": "hMaRCVoTqF0R"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = JupyterDash(__name__)\n",
        "\n",
        "\n",
        "##############################################\n",
        "# APP LAYOUT\n",
        "##############################################\n",
        "\n",
        "app.layout = html.Div([\n",
        "          html.H1('Movie Recommendation'),\n",
        "          \n",
        "          html.Div(children=[dcc.Dropdown(id=\"movie-lists\",\n",
        "                                          options=movie_lists,\n",
        "                                          multi=False)]\n",
        "                   ),\n",
        "          html.Div(children=[html.Div(children=[html.H3(id='focused-movie',\n",
        "                                                        style={'margin': 'auto'}),\n",
        "                                                html.Div(id='focused-poster')],\n",
        "                                      style={'textAlign': 'center', 'flexBasis': '100%'}),\n",
        "                             html.Div(id='similar-movies',\n",
        "                                      style={'textAlign': 'center', 'flexBasis': '100%', 'display': 'flex', 'width': '100%', 'justifyContent': 'space-around'})\n",
        "                   ],\n",
        "                   style={'display': 'flex', 'width': '90%', 'justifyContent': 'space-around'}\n",
        "                   ),\n",
        "          \n",
        "\n",
        "          html.H2(\"We will recommend movies that are close to the story you describe.\"),\n",
        "          html.Div(dcc.Input(id='overview', type='text',\n",
        "                             placeholder='Describe the movie you want to see... (keyword, overview, story)',\n",
        "                             value='Ice magic princess who lives with sister',\n",
        "                             style={'width': '80%'})\n",
        "          ),\n",
        "          html.Button(id='submit-button-state', n_clicks=0, children='Submit'),\n",
        "          html.Div(id=\"movie\",\n",
        "                   children=[html.Div(children=[html.H3(id='moviename1', style={'marginBottom': 0}),\n",
        "                                                html.H4(id='moviescore1', style={'margin': 'auto'}),\n",
        "                                                html.Div(id='poster1')],\n",
        "                                      style={'textAlign': 'center', 'flexBasis': '100%'}),\n",
        "                             html.Div(children=[html.H3(id='moviename2', style={'marginBottom': 0}),\n",
        "                                                html.H4(id='moviescore2', style={'margin': 'auto'}),\n",
        "                                                html.Div(id='poster2')],\n",
        "                                      style={'textAlign': 'center', 'flexBasis': '100%'}),\n",
        "                             html.Div(children=[html.H3(id='moviename3', style={'marginBottom': 0}),\n",
        "                                                html.H4(id='moviescore3', style={'margin': 'auto'}),\n",
        "                                                html.Div(id='poster3')],\n",
        "                                      style={'textAlign': 'center', 'flexBasis': '100%'})],\n",
        "                   style={'display': 'flex', 'width': '90%', 'justifyContent': 'space-around'}\n",
        "                   )\n",
        "          ])\n",
        "\n",
        "\n",
        "##############################################\n",
        "# APP CALLBACKS\n",
        "##############################################\n",
        "\n",
        "@app.callback([Output('moviename1', 'children'),\n",
        "               Output('moviename2', 'children'),\n",
        "               Output('moviename3', 'children'),\n",
        "               Output('moviescore1', 'children'),\n",
        "               Output('moviescore2', 'children'),\n",
        "               Output('moviescore3', 'children'),\n",
        "               Output('poster1', 'children'),\n",
        "               Output('poster2', 'children'),\n",
        "               Output('poster3', 'children')],\n",
        "              Input('submit-button-state', 'n_clicks'),\n",
        "              State('overview', 'value'),\n",
        "              prevent_initial_call=True)\n",
        "def recommendation(_, text):\n",
        "  topn = 3\n",
        "  prepared_text = prepare_text(text)\n",
        "  embedded_text = [list(sbert_model.encode(prepared_text))]\n",
        "\n",
        "  similarity = cosine_similarity(embedded_text, document_embeddings)[0]\n",
        "  similar_idxs = np.argsort(similarity)[::-1]\n",
        "\n",
        "  similarity_scores = list(similarity[similar_idxs[:topn]])\n",
        "  recommended_movies = list(movie_lists[similar_idxs[:topn]])\n",
        "  recommended_posters = list(poster_lists[similar_idxs[:topn]])\n",
        "              \n",
        "  scores = [' (' + str(round(similarity_scores[i]*100, 2)) + '%)' for i in range(topn)]\n",
        "\n",
        "  poster_divs = [html.Img(style={'maxHeight': '400px'}, src=url) for url in recommended_posters]\n",
        "\n",
        "  return recommended_movies + scores + poster_divs\n",
        "\n",
        "\n",
        "@app.callback([Output('focused-movie', 'children'),\n",
        "               Output('focused-poster', 'children'),\n",
        "               Output('similar-movies', 'children')],\n",
        "              Input('movie-lists', 'value'))\n",
        "def similarity(focus_movie):\n",
        "  topn = 3\n",
        "\n",
        "  focus_movie_idx = list(movie_lists).index(focus_movie)\n",
        "  focus_poster_url = poster_lists[focus_movie_idx]\n",
        "  focus_poster = html.Img(style={'maxHeight': '400px'}, src=focus_poster_url)\n",
        "\n",
        "  pairwise = pairwise_similarities[focus_movie_idx]  \n",
        "  similar_idxs = np.argsort(pairwise)[::-1]\n",
        "  similar_scores = list(pairwise[similar_idxs[1:topn+1]])\n",
        "  similar_movies = list(movie_lists[similar_idxs[1:topn+1]])\n",
        "  similar_posters = list(poster_lists[similar_idxs[1:topn+1]])\n",
        "\n",
        "  similar_div_list = [html.Div(children=[html.H3(similar_movies[i],\n",
        "                                                 style={'marginBottom': 0}),\n",
        "                                         html.H4('(' + str(round(similar_scores[i]*100, 2)) + '%)',\n",
        "                                                 style={'margin': 'auto'}),\n",
        "                                         html.Div(html.Img(style={'maxHeight': '250px'},\n",
        "                                                      src=similar_posters[i]))\n",
        "                                         ],\n",
        "                               style={'textAlign': 'center', 'flexBasis': '100%', 'margin': '10px'}) for i in range(topn)]\n",
        "\n",
        "  similar_div = html.Div(children=[html.H5(\"Similar movies\"),\n",
        "                                   html.Div(children=[similar_div_list[0],\n",
        "                                                      similar_div_list[1],\n",
        "                                                      similar_div_list[2]],\n",
        "                                            style={'display': 'flex', 'width': '90%', 'justifyContent': 'space-around'}),\n",
        "                                   ]\n",
        "                         )\n",
        "\n",
        "  return focus_movie, focus_poster, similar_div\n",
        "\n",
        "\n",
        "app.run_server('inline', debug=True)\n"
      ],
      "metadata": {
        "id": "7_9h5HsyxhNC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "outputId": "88841977-21dc-4a07-fda2-5d918f8f6d85"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PxnRgJSi75jd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}